# botapp/ai_client.py
from __future__ import annotations

import asyncio
import logging
import os
from dataclasses import dataclass
from pathlib import Path

import httpx

from botapp.ai_memory import fetch_examples, format_examples_block

logger = logging.getLogger(__name__)

OPENAI_API_KEY_ENV = "OPENAI_API_KEY"
OPENAI_BASE_URL_ENV = "OPENAI_BASE_URL"
OPENAI_MODEL_ENV = "OPENAI_MODEL"
OPENAI_TIMEOUT_S_ENV = "OPENAI_TIMEOUT_S"
ITOM_QNA_DIGEST_PATH_ENV = "ITOM_QNA_DIGEST_PATH"

DEFAULT_MODEL = "gpt-4o-mini"
DEFAULT_TIMEOUT_S = 35.0

ULYANOVA_PATH_ENV = "ULYANOVA_PATH"
DEFAULT_ULYANOVA_PATHS = (
    "data/ulyanova.txt",
    "data/ulyanova.md",
    "ulyanova.txt",
    "ulyanova.md",
)


_style_cache: dict[str, str | float | Path | None] = {"path": None, "mtime": None, "text": None}


class AIClientError(RuntimeError):
    pass


def _get_env(name: str, default: str = "") -> str:
    return (os.getenv(name) or default).strip()


def _load_ulyanova_text() -> str:
    p = _get_env(ULYANOVA_PATH_ENV)
    candidates: list[str] = []
    if p:
        candidates.append(p)
    candidates.extend(DEFAULT_ULYANOVA_PATHS)

    for c in candidates:
        try:
            path = Path(c)
            if path.exists() and path.is_file():
                txt = path.read_text(encoding="utf-8", errors="replace").strip()
                if txt:
                    return txt
        except Exception:
            continue
    return ""


def _style_guide_path() -> Path:
    env_path = _get_env(ITOM_QNA_DIGEST_PATH_ENV)
    if env_path:
        try:
            return Path(env_path).expanduser().resolve()
        except Exception:
            pass
    base = Path(__file__).resolve().parents[1]
    return (base / "data" / "itom_qna_digest.txt").resolve()


def _load_style_guide() -> str:
    global _style_cache
    path = _style_guide_path()

    try:
        mtime = path.stat().st_mtime
    except Exception:
        return ""

    cached_path = _style_cache.get("path")
    cached_mtime = _style_cache.get("mtime")
    if cached_path == path and cached_mtime == mtime:
        cached_text = _style_cache.get("text")
        return cached_text or ""

    try:
        text = path.read_text(encoding="utf-8", errors="replace").strip()
    except Exception:
        return ""

    _style_cache = {"path": path, "mtime": mtime, "text": text}
    logger.info("ITOM style guide loaded: %d chars from %s", len(text), path)
    return text


def _normalize(s: str) -> str:
    return (s or "").strip()


def _clamp_text(s: str, n: int) -> str:
    s = _normalize(s)
    if len(s) <= n:
        return s
    return s[: max(0, n - 1)].rstrip() + "‚Ä¶"


def _maybe_add_soft_emoji(reply: str, rating: int | None) -> str:
    if not reply:
        return reply
    try:
        r = int(rating) if rating is not None else None
    except Exception:
        r = None
    if r is None or r < 4:
        return reply

    friendly_emojis = ["üòä", "üôÇ", "‚ù§Ô∏è"]
    if any(e in reply for e in friendly_emojis):
        return reply

    return reply.rstrip() + " " + friendly_emojis[0]


@dataclass
class OpenAIConfig:
    api_key: str
    base_url: str
    model: str
    timeout_s: float


def _config() -> OpenAIConfig:
    api_key = _get_env(OPENAI_API_KEY_ENV)
    if not api_key:
        raise AIClientError(f"–ù–µ –∑–∞–¥–∞–Ω {OPENAI_API_KEY_ENV} (–∫–ª—é—á OpenAI)")

    base_url = _get_env(OPENAI_BASE_URL_ENV, "https://api.openai.com/v1").rstrip("/")
    model = _get_env(OPENAI_MODEL_ENV, DEFAULT_MODEL)
    try:
        timeout_s = float(_get_env(OPENAI_TIMEOUT_S_ENV, str(DEFAULT_TIMEOUT_S)))
    except Exception:
        timeout_s = DEFAULT_TIMEOUT_S

    return OpenAIConfig(api_key=api_key, base_url=base_url, model=model, timeout_s=timeout_s)


async def _chat_completion(
    *,
    system: str,
    user: str,
    temperature: float = 0.4,
    max_tokens: int = 350,
) -> str:
    cfg = _config()
    url = f"{cfg.base_url}/chat/completions"
    headers = {"Authorization": f"Bearer {cfg.api_key}", "Content-Type": "application/json"}
    payload = {
        "model": cfg.model,
        "temperature": float(temperature),
        "max_tokens": int(max_tokens),
        "messages": [
            {"role": "system", "content": _normalize(system)},
            {"role": "user", "content": _normalize(user)},
        ],
    }

    retries = 2
    last_err: Exception | None = None

    async with httpx.AsyncClient(timeout=httpx.Timeout(cfg.timeout_s)) as client:
        for attempt in range(retries + 1):
            try:
                r = await client.post(url, headers=headers, json=payload)
                r.raise_for_status()
                data = r.json()
                choices = data.get("choices") if isinstance(data, dict) else None
                if isinstance(choices, list) and choices:
                    msg = choices[0].get("message") if isinstance(choices[0], dict) else None
                    content = (msg or {}).get("content") if isinstance(msg, dict) else None
                    if isinstance(content, str):
                        return _normalize(content)
                raise AIClientError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç OpenAI: {data}")
            except Exception as exc:
                last_err = exc
                if attempt >= retries:
                    break
                await asyncio.sleep(0.6 + attempt * 0.8)

    raise AIClientError(f"OpenAI –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞–ª—Å—è: {last_err}")


def _base_rules() -> str:
    ulyanova = _load_ulyanova_text()
    if ulyanova:
        return (
            "–ù–∏–∂–µ ‚Äî –±–∞–∑–∞ –ø—Ä–∞–≤–∏–ª/–ø–æ–¥—Ö–æ–¥–æ–≤ (–£–ª—å—è–Ω–æ–≤–∞). –°–æ–±–ª—é–¥–∞–π –∏—Ö –≤—Å–µ–≥–¥–∞.\n\n"
            f"{ulyanova}\n"
        )
    return ""


def _style_guide_block() -> str:
    style = _load_style_guide()
    if not style:
        return ""
    return "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å—Ç–∏–ª—è –∏ —à–∞–±–ª–æ–Ω–æ–≤ ITOM:\n" + style


def _examples_block(kind: str, *, input_text: str, sku: str | None = None, limit: int = 6) -> str:
    examples = fetch_examples(kind=kind, input_text=input_text, sku=sku, limit=limit)
    return format_examples_block(examples)


async def generate_review_reply(
    *,
    review_text: str,
    product_name: str | None = None,
    sku: str | None = None,
    rating: int | None = None,
    previous_answer: str | None = None,
    user_prompt: str | None = None,
) -> str:
    sys = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø—Ä–æ–¥–∞–≤—Ü–∞ –Ω–∞ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–µ Ozon. "
        "–ü–∏—à–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Ç–∑—ã–≤—ã –ø–æ-—Ä—É—Å—Å–∫–∏, –≤–µ–∂–ª–∏–≤–æ, –±–µ–∑ –≤–æ–¥—ã, –±–µ–∑ –æ–±–µ—â–∞–Ω–∏–π —Ç–æ–≥–æ, —á—Ç–æ –Ω–µ–ª—å–∑—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å. "
        "–ù–µ —É–ø–æ–º–∏–Ω–∞–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã Ozon –∏ –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã –∏–ª–∏ —Å—Ä–æ–∫–∏ –¥–æ—Å—Ç–∞–≤–∫–∏. "
        "–ï—Å–ª–∏ –æ—Ç–∑—ã–≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Å–Ω–∞—á–∞–ª–∞ —Å–æ—á—É–≤—Å—Ç–≤–∏–µ, –∑–∞—Ç–µ–º –∫–æ—Ä–æ—Ç–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –∑–∞—Ç–µ–º –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ —É—Ç–æ—á–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏. "
        "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –¥–æ—Å—Ç–∞–≤–∫—É ‚Äî –Ω–∞–ø–æ–º–Ω–∏, —á—Ç–æ –¥–æ—Å—Ç–∞–≤–∫—É –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç Ozon, –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞—Ç."
        "\n\n"
        + _base_rules()
    )

    style_block = _style_guide_block()
    if style_block:
        sys = sys + "\n\n" + style_block

    user = []
    if product_name:
        user.append(f"–¢–æ–≤–∞—Ä: {product_name}")
    if rating is not None:
        user.append(f"–û—Ü–µ–Ω–∫–∞: {rating}/5")
    user.append("–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:")
    user.append(review_text or "")

    if previous_answer:
        user.append("\n–¢–µ–∫—É—â–∏–π —á–µ—Ä–Ω–æ–≤–∏–∫ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å/–ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å):")
        user.append(previous_answer)

    if user_prompt:
        user.append("\n–ü–æ–∂–µ–ª–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É (–ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞):")
        user.append(user_prompt)

    examples_block = _examples_block("review", input_text=review_text, sku=sku)
    if examples_block:
        sys = sys + "\n\n" + examples_block

    user.append(
        "\n–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ–¥–∏–Ω –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤ –∫–Ω–æ–ø–æ–∫ –∏ –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤). "
        "–ú–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤, –µ—Å–ª–∏ –º–æ–∂–Ω–æ ‚Äî –∫–æ—Ä–æ—á–µ."
    )

    reply = await _chat_completion(system=sys, user="\n".join(user), temperature=0.5, max_tokens=260)
    return _maybe_add_soft_emoji(reply, rating)


async def generate_answer_for_question(
    question_text: str,
    *,
    product_name: str | None = None,
    sku: str | None = None,
    existing_answer: str | None = None,
    user_prompt: str | None = None,
) -> str:
    sys = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø—Ä–æ–¥–∞–≤—Ü–∞ –Ω–∞ Ozon. "
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ –¥–µ–ª—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö –æ–ø—Ä–∞–≤–¥–∞–Ω–∏–π. "
        "–ù–µ –æ–±–µ—â–∞–π —Å—Ä–æ–∫–∏ –¥–æ—Å—Ç–∞–≤–∫–∏ (—ç—Ç–æ –∑–æ–Ω–∞ Ozon), –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π –ø–µ—Ä–µ–π—Ç–∏ –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä—ã. "
        "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî –∑–∞–¥–∞–π 1 —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ü–µ. "
        "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–µ—Ç–∞–ª–∏ —Ç–æ–≤–∞—Ä–∞ –∏ –Ω–µ –æ–±–µ—â–∞–π –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–µ."
        "\n\n"
        + _base_rules()
    )

    style_block = _style_guide_block()
    if style_block:
        sys = sys + "\n\n" + style_block

    user = []
    if product_name:
        user.append(f"–¢–æ–≤–∞—Ä: {product_name}")
    user.append("–í–æ–ø—Ä–æ—Å –ø–æ–∫—É–ø–∞—Ç–µ–ª—è:")
    user.append(question_text or "")

    if existing_answer:
        user.append("\n–¢–µ–∫—É—â–∏–π —á–µ—Ä–Ω–æ–≤–∏–∫ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å):")
        user.append(existing_answer)

    if user_prompt:
        user.append("\n–ü–æ–∂–µ–ª–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:")
        user.append(user_prompt)

    examples_block = _examples_block("question", input_text=question_text, sku=sku)
    if examples_block:
        sys = sys + "\n\n" + examples_block

    user.append(
        "\n–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ–¥–∏–Ω –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç. "
        "–ú–∞–∫—Å–∏–º—É–º 450 —Å–∏–º–≤–æ–ª–æ–≤, —Ç–æ–Ω ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏ –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π."
    )

    return await _chat_completion(system=sys, user="\n".join(user), temperature=0.4, max_tokens=240)


async def generate_chat_reply(*, messages_text: str, user_prompt: str | None = None) -> str:
    sys = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø—Ä–æ–¥–∞–≤—Ü–∞ –≤ —á–∞—Ç–µ Ozon. "
        "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –∫–∞–∫ –ø—Ä–æ–¥–∞–≤–µ—Ü. "
        "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª–Ω–æ—Å—Ç—å—é; –¥–∞–π –æ–¥–∏–Ω –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é –ø–æ–∫—É–ø–∞—Ç–µ–ª—è. "
        "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî —É—Ç–æ—á–Ω–∏ –¥–µ—Ç–∞–ª–∏ –æ–¥–Ω–∏–º –≤–æ–ø—Ä–æ—Å–æ–º. "
        "–ù–µ —É–ø–æ–º–∏–Ω–∞–π, —á—Ç–æ —Ç—ã –ò–ò. –ù–µ –æ–±–µ—â–∞–π —Å—Ä–æ–∫–∏ –¥–æ—Å—Ç–∞–≤–∫–∏ –∏ –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–µ—Ç–∞–ª–∏ –∑–∞–∫–∞–∑–∞."
        "\n\n"
        + _base_rules()
    )

    style_block = _style_guide_block()
    if style_block:
        sys = sys + "\n\n" + style_block

    user = []
    if user_prompt:
        user.append("–ü–æ–∂–µ–ª–∞–Ω–∏—è/–ø—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:")
        user.append(user_prompt)
        user.append("")

    user.append("–ü–µ—Ä–µ–ø–∏—Å–∫–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç):")
    user.append(_clamp_text(messages_text or "", 7000))

    examples_block = _examples_block("chat", input_text=messages_text, sku=None, limit=4)
    if examples_block:
        sys = sys + "\n\n" + examples_block
    user.append(
        "\n–°—Ñ–æ—Ä–º–∏—Ä—É–π –û–î–ò–ù –æ—Ç–≤–µ—Ç –ø—Ä–æ–¥–∞–≤—Ü–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ BUYER. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –º–µ—Ç–∫–∏ BUYER/SELLER –≤ –æ—Ç–≤–µ—Ç."
    )

    return await _chat_completion(system=sys, user="\n".join(user), temperature=0.45, max_tokens=220)


__all__ = [
    "AIClientError",
    "generate_review_reply",
    "generate_answer_for_question",
    "generate_chat_reply",
]
